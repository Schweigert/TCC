\setlength\abovedisplayskip{0pt} \setlength\belowdisplayskip{0pt}
\setlength\abovedisplayshortskip{0pt} \setlength\belowdisplayshortskip{0pt}

\chapter{Fundamentação Teórica} \label{cap1}
Neste capítulo são apresentados os conceitos
básicos e a fundamentação teórica necessária para o entendimento e abordagem do
problema do escalonamento de tripulações.

\section{Programação Linear}

A programação linear consiste na modelagem e solução de problemas descritos com
uma função objetivo linear sujeita a múltiplas restrições lineares. A forma
genérica de um problema de programação linear é\cite{dantzig1955generalized}:

\begin{align} \label{funcao_obj} \text{maximizar} \: z &= \sum_{j=1}^{p}
    c_jx_j, \intertext{sujeito à:} \label{restricoes} \sum_{j=1}^{p} a_{ij} x_j
    \leq b_i, i &= 1, 2, \ldots, q \\ \label{restricoes_triviais} x_j \geq 0, j
    &= 1, 2, \ldots, p, \end{align}

onde $c_j, a_{ij}$ e $b_i$ são números reais que definem o problema e $x_j$
para $j=1, 2, \ldots, p$ são as variáveis de decisão. A
função~\eqref{funcao_obj} é denominada de função objetivo, a qual deve ser
maximizada. Note que o máximo de $f(x)$ é o mínimo de $-f(x)$ com o sinal
oposto max $f(x) = - \text{min} f(x)$. As inequações em~\eqref{restricoes}
representam um conjunto de $q$ restrições lineares que restringem o espaço de
busca para um poliedro convexo. O máximo da função deve estar contido dentro
deste poliedro. As desigualdades em~\eqref{restricoes_triviais} são denominadas
de restrições triviais ou de não negatividade.

Cada restrição em~\eqref{restricoes} pode ser convertida para restrição de
igualdade utilizando-se uma variável extra, denominada de variável de folga. A
utilização dá-se por:

\[ \label{restricoes} \sum_{j=1}^{p} a_{ij} x_j \leq b_i \Leftrightarrow
\begin{cases} \Sigma_{j=1}^{p} a_{ij} x_j + x_{p+i} = b_i\\ x_{p+i} \geq 0
\end{cases} \]

é possível ainda utilizar duas igualdades para representar uma igualdade:

\[ \label{restricoes} \sum_{j=1}^{p} a_{ij} x_j = b_i \Leftrightarrow
\begin{cases} \Sigma_{j=1}^{p} a_{ij} x_j \leq b_i\\ \Sigma_{j=1}^{p} a_{ij}
x_j \geq b_i \end{cases} \]

Para um problema qualquer de programação linear com restrições de desigualdades
e igualdades, sempre é possível reestruturá-lo através da adição de variáveis
de folga, para que o problema passe a ter apenas igualdades. Portanto, todo e
qualquer problema de programação linear pode ser expresso como:

\begin{align} \label{fo} \text{maximizar} \: z &= \sum_{j=1}^{n} c_jx_j
    \intertext{sujeito à:} \label{res} \sum_{j=1}^{n} a_{ij} x_j \leq b_i, i &=
    1, 2, \ldots, m \\ \label{res_t} x_j \geq 0, j &= 1, 2, \ldots, n
    \intertext{que é equivalente à:} \label{fo2} \text{maximizar} \: z &= cx
\intertext{sujeito à:} \label{res2} Ax &= b \\ \label{res_t2} x &\geq 0,
\end{align}

onde $c^T \in \mathbb{R}^n$, $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, $A \in
\mathbb{R}^{m \times n}$ e $a_j \in \mathbb{R}^m$.

Baseado nas restrições~\eqref{res2} e~\eqref{res_t2}, pode-se descrever o
problema como encontrar $x \in X$, onde $X = \{ x \in \mathbb{R}^n | Ax = b, x
\geq 0 \}$, tal que $cx$ seja maximizado. Ao conjunto $X$ dá-se o nome de
região factível ou espaço de busca. Para um $x \in X$ qualquer, diz-se que $x$
é uma solução factível.  Para um $x^* \in X$, se $cx^* \geq cx \forall x \in
X$, então $x^*$ é a solução ótima do problema. O espaço de busca para um
problema de programação linear é sempre um poliedro convexo, se interpretado
geometricamente, e a solução ótima para o mesmo sempre está em um vértice do
poliedro.

Um problema de programação linear pode ser ainda expresso em uma forma
matricial, ou em forma de conjunto. Suas representações são respectivamente:

\begin{align} \label{lp_fo} \text{maximizar} \: z &= cx \intertext{sujeito à:}
\label{lp_r} Ax &= b \\ \label{lp_tr} x &\geq 0 \intertext{} \label{lp_set}
\text{max } \{ cx : Ax &\leq b, x \geq 0 \} \end{align}

\subsection{Métodos de solução}

Considerando o conjunto $X$ descrito na seção anterior, o objetivo de efetuar a
modelagem de um problema utilizando-se programação linear é utilizar recursos
matemáticos e algorítmicos para resolver o modelo, e por conseguinte o problema
original. Esta seção apresenta os três principais métodos encontrados na
literatura durante o desenvolvimento deste trabalho: o método simplex; o método
dos elipsoides; o método do ponto interior.

%\subsubsection{Simplex}

O método Simplex~\cite{dantzig1990origins} foi apresentado em 1947 por George
B. Dantzig com o objetivo de resolver o problema de programação linear. O
método consiste em encontrar uma solução factível ao problema, e iterativamente
mover para uma solução melhor ou igual que a atual. Considerando que a solução
está em um vértice do poliedro, é necessário apenas explorar os vértices. Tendo
em vista que existe um número finito de vértices para um poliedro descrito por
um conjunto finito de restrições lineares (que geometricamente correspondem a
hiperplanos), fica claro que o simplex converge em um número finito de passos.
Apesar de que na média o simplex resolve o problema em um número polinomial de
passos, em 1972 foi apresentado uma prova de que o método simplex no seu pior
caso é exponencial~\cite{klee1972good}.  Klee e Minty apresentaram um politopo
especialmente projetado para que o método simplex leve um número exponencial de
passos, o politopo é denominado de Cubo de Kleen-Minty.

%\subsubsection{Método dos elipsoides}

Em decorrência da descoberta do cubo de Klee-Minty, diversos pesquisadores
iniciaram um estudo em busca de um método capaz de resolver o problema de
programação linear em tempo polinomial. Um dos primeiros trabalhos apresentados
na literatura propondo um algoritmo polinomial foi o método dos
elipsoides~\cite{khachian}. O método consiste em criar um elipsoide que englobe
a solução ótima e reduzi-lo sequencialmente de modo que a solução ótima sempre
esteja dentro do elipsoide. O método possui, em teoria, convergência garantida
em tempo polinomial. No entanto, na prática o método apresenta um desempenho
inferior ao simplex, como problemas de instabilidade numérica. O método
dos elipsoides demonstrou que diversos problemas podem ser resolvidos em tempo
polinomial~\cite{Grotschel1981}.

%\subsubsection{Método do ponto interior}

Em 1984 foi proposto um novo método polinomial para a solução do problema da
programação linear, denominado de método do ponto interior\cite{potra2000interior}.
Este método consiste em a partir de uma solução factível centro do politopo, perturba-la
até que a mesma convirja para o ponto ótimo. O método do ponto interior também é
referenciado na literatura como método das barreiras, já que as restrições
lineares são reescritas como funções assintóticas que tendem ao infinito quando
a restrição linear original é violada. O método do ponto interior apresentou um
desempenho comparável ao do simplex, e é especialmente aplicado em problemas de
larga escala.


\subsection{Dualidade}

Considerando o problema de programação linear apresentado
em~\eqref{funcao_obj},~\eqref{restricoes} e~\eqref{restricoes_triviais}, que a
partir de agora será denominado de problema primal,

\begin{align} \text{maximizar} \: z = \sum_{j=1}^{p} c_jx_j, \intertext{sujeito
a:} \label{rp} \sum_{j=1}^{p} a_{ij} x_j \leq b_i, i = 1, 2, \ldots, q \\
\label{} x_j \geq 0, j = 1, 2, \ldots, p, \end{align}.

O problema dual é construído atribuindo-se uma variável $u_i, i = 1, 2, \ldots,
q$ a cada restrição em~\eqref{rp} e definindo o problema como:

\begin{align} \text{minimizar} \: d = \sum_{i=1}^{q} b_iu_i, \intertext{sujeito
a:} \label{} \sum_{i=1}^{q} a_{ij} u_i \leq c_j, i = 1, 2, \ldots, p \\
\label{} u_i \geq 0, i = 1, 2, \ldots, q, \end{align}

O problema primal e dual em sua forma matricial são dados por:

\begin{align} \label{} \text{maximizar} \: z &= cx \intertext{sujeito à:}
    \label{prnt} Ax &= b \\ \label{prt} x &\geq 0, \intertext{e} \label{}
    \text{minimizar} \: d &= ub \intertext{sujeito à:} \label{drnt} uA^t &= c \\
    \label{drt} u &\geq 0,
\end{align}

À partir do problema primal e dual, conforme demonstrado
em~\cite{maculan2006otimizaccao}, segue que:

\begin{enumerate} \label{x} \item Se $\overline{x}$ satisfaz~\eqref{prnt}
        e~\eqref{prt} e $\overline{u}$ satisfaz~\eqref{drnt} e~\eqref{drt},
    então $c\overline{x} \leq \overline{u}b$; \item Se $\overline{x}$ e
        $\overline{u}$ forem \textbf{soluções factíveis} do problema primal e dual,
        respectivamente, e $c\overline{x} = \overline{u}b$, então
        $\overline{x}$ é a \textbf{solução ótima} do problema primal e $\overline{u}$ é
        a solução ótima do problema dual;
    %\item Se o problema primal tiver seu espaço de busca ilimitado, então o
        %problema dual é infactível, e vice-versa;
    \item Se $\tilde{x}$ é a \textbf{solução ótima} do problema primal e $\tilde{u}$ é a
        \textbf{solução ótima} do problema dual, então $c\tilde{x} = \tilde{u}b$.
\end{enumerate}

O primeiro item é conhecido como teorema da \textbf{dualidade fraca}, e sua implicação é
que uma solução dual é um limite superior de otimalidade para o problema
primal.  O segundo e terceiro item constituem o teorema da \textbf{dualidade forte}, que
pode ser utilizado para provar que uma solução de um problema primal é a ótima.
%Segue ainda que apenas uma das três alternativas abaixo é verdadeira para um
%problema primal e o seu problema dual associado:

%\begin{itemize} \item Ambos os problemas tem um espaço de busca vazio; \item Um
%deles tem o seu espaço de busca vazio e o outro ilimitado; \item Ambos possuem
%o mesmo valor na função objetivo e possuem soluções ótimas finitas;
%\end{itemize}

\section{Programação Linear Inteira}

%Na prática a programação inteira não é o suficiente para representar diversos
%problemas, visto que o valor fracionário da soluçao (variáveis) pode não ser condizente
%a a realidade.

%A fim de ilustrar o problema de variáveis frácionarias, considerw um
%grafo, por exemplo, pode-se modelar um problema de PLI onde $x_{ij} = 1$ se um vértice
%$i$ esta ligada a um vértice $j$ e $0$ caso contrário.  Ao resolver o modelo, pode ocorrer
%que o valor da variável venha a ser algo entre $0$ e $1$, que não possui
%representação no problema.  Em um caso prático, uma variável com um valor
%fracionário pode indicar que é necessário que $3.19$ ônibus devem ser alocados
%para uma determinada rota, algo que não faz sentido. Fica claro então que são
%necessários recursos extras para representar problemas onde pelo menos uma
%variável deve ser inteira.

A Programação Linear Inteira (PLI) trata de modelar problemas onde existem
variáveis inteiras. A forma genérica de um problema dá-se de forma análoga a de
um problema de programação linear:

\begin{align} \text{} \: z = \sum_{j=1}^{p} c_jx_j, \intertext{sujeito à:}
\label{} \sum_{j=1}^{p} a_{ij} x_j \leq b_i, i &= 1, 2, \ldots, q \\
\label{interg} x_j \geq 0 \text{ e } x_j \in \mathbb{Z}, j &= 1, 2, \ldots, p,
\end{align}

Nota-se que o modelo acima é exatamente igual a um problema genérico de
programação linear, exceto pela restrição em~\eqref{interg}, que faz com que os
valores de $x_j$ sejam inteiros. Esta restrição é denominada de restrição de
integralidade.

Outros problemas podem precisar de soluções que sejam inteiras e fracionárias, estes
problemas são denominados de problemas de programação linear mista (PLIM) e
possuem uma forma genérica de:

\begin{align} \text{} \: z = \sum_{j=1}^{p} c_jx_j, \intertext{sujeito à:}
    \label{plim_r} \sum_{j=1}^{p} a_{ij} x_j \leq b_i, i &= 1, 2, \ldots, q \\
    \label{plim_v} \sum_{j=1}^{r} g_{ij} y_j \leq d_i, i &= 1, 2, \ldots, q \\
    \label{} x_j \geq 0, j &= 1, 2, \ldots, p, \\ \label{plim_int} y_j \geq 0
\text{ e } y_j \in \mathbb{Z}, j &= 1, 2, \ldots, r \end{align}

A restrição~\eqref{plim_r} representa as variáveis fracionárias do modelo, a
restrição~\eqref{plim_v} as variáveis inteiras e~\eqref{plim_int} é a restrição
de integralidade.

Existem ainda problemas na qual é utilizado apenas variáveis binárias, a estes
problemas dá-se o nome de problema de programação linear inteira binária
(PLIB).  Eles possuem a forma genérica de:

\begin{align} \text{} \: z = \sum_{j=1}^{p} c_jx_j, \intertext{sujeito à:}
\label{} \sum_{j=1}^{p} a_{ij} x_j \leq b_i, i &= 1, 2, \ldots, q \\
\label{pbin} x_j \in \{0, 1\}, j &= 1, 2, \ldots, p, \end{align}

As restrições são iguais a de um problema de programação linear, exceto para a
restrição~\eqref{pbin}, que restringe o domínio das variáveis para valores de
$0$ ou $1$.

A PLI e suas variantes (PLIM e PLIB) são problemas $\mathcal{NP}$-completos,
conforme demonstrados por~\cite{karp1972} e~\cite{papadimitriou1981complexity}.
Sabe-se ainda que a PLI é um problema $\mathcal{NP}$-completo forte, portanto, é
improvável que exista um algoritmo pseudo-polinomial capaz de
resolvê-lo~\cite{garey1978strong}.

Considere o problema de PLI a seguir:

\begin{equation} \label{pli_example} A = \begin{pmatrix} -1 &  2 \\ 5 &  1 \\
    -2 & -2 \end{pmatrix}, \quad b = \begin{pmatrix} 4 \\ 20 \\ -7
        \end{pmatrix}, \quad c^T = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
\end{equation}


\begin{figure}[!htb]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \begin{tabular}{r l}
            função objetivo & = $6.9090\ldots$ \\
            $x_1$           & = $3.2727\ldots$ \\
            $x_2$           & = $3.6363\ldots$
        \end{tabular}
        \captionof{table}{Solução da PLI}
        \label{rrrr}
    \end{minipage}
%
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{../figuras/pli.pdf}
        \caption{Espaço de busca do problema \eqref{pli_example}} \label{fig:name}
        \label{espacobusca}
    \end{minipage}
\end{figure}

Resolvendo-o obtém-se o resultado apresentado na figura~\ref{rrrr}.
Considerando que o problema de PLI admite apenas variáveis com valores
inteiros, o resultado acima é inválido como resposta para o problema.
Observando-se a figura~\ref{fig:name}, que contém o espaço de busca do problema
em questão, pode-se observar que os pontos que são factíveis ao problema de PLI
são um subconjunto do espaço de busca de um problema de programação linear com
as mesmas restrições. Observa-se ainda que a solução ótima do problema de PLI
não coincide com a solução ótima do problema de programação linear. Portanto, é
necessário métodos específicos para resolver problemas de PLI.

\subsection{Relaxação Linear}

O conceito de relaxação no contexto de otimização corresponde a remover alguma
restrição do problema. O relaxamento de um problema tem como objetivo torná-lo
mais fácil de resolver. Dentre as diversas restrições que podem ser relaxadas, uma delas
é a restrição de integralidade, que se removida leva a uma relaxação linear.
Considerando que o problema de PLI genérico é $\mathcal{NP}$-completo, a
relaxação linear torná-lo um problema $\mathcal{P}$, que pode ser resolvido mais
rapidamente\cite{garey1978strong}.

Seja $x^*$ a solução ótima de um problema de maximização PLI e $\hat{x}^*$ a
solução ótima da relaxação do mesmo, segue que $x^* \leq \hat{x}^*$. Portanto a
relaxação linear é um limite superior de otimalidade para um problema de PLI
qualquer. O mesmo é válido para PLIM e PLIB.

O espaço de busca de um problema de PLI consiste em um conjunto de pontos com
coordenadas inteiras. A figura~\ref{espacobusca} contém o conjunto de pontos
que satisfazem o problema original. O contorno que envolve os pontos consiste
na relaxação linear, onde a restrição de integralidade foi desconsiderada.
Pode-se notar que a área de busca aumentou e a solução ótima da relaxação é
maior do que o problema original.

\subsection{Relaxação Combinatória}

Para problemas combinatórios (discretos), a relaxação combinatória consiste em
remover uma ou mais restrições de um problema. Igualmente a relaxação linear, a
relaxação combinatória oferece um limite superior de otimalidade para um
problema de PLI, no entanto a relaxação combinatória não necessariamente torna
o problema mais fácil de se resolver, no sentido de diminuir a complexidade.

Um exemplo de relaxação combinatória é a remoção da restrição de sub-tours no
problema do caixeiro viajante, que transforma-o em um problema de designação. O
problema de designação é um problema $\mathcal{P}$. Na prática utiliza-se esta
relaxação para resolver-se o problema do caixeiro
viajante~\cite{laporte1992traveling}.

\subsection{Métodos de solução de PLI}

Um dos primeiros métodos para solução de problemas de programação inteira foi
proposto por~\cite{gomory1960solving,gomory1960algorithm}. O método veio a ser
denominado de método de planos de cortes, que consiste em gerar hiperplanos
(restrições) que removem o ponto ótimo da relaxação linear sem remover o ótimo
da PLI. Esta sequencia de cortes faz com que a solução ótima da relaxação linear
seja a mesma do problema original.

Utilizando-se planos de corte pode-se
resolver problemas de PLI apenas com o SIMPLEX. O método funciona teoricamente,
porém na prática o método apresenta problemas de instabilidade numérica,
tornando-o inviável. Existem estudos que propuseram mudanças para viabilizar
o métodos~\cite{cook2009numerically}. \cite{zanette2011lexicography} apresenta
o uso do simplex lexicográfico para evitar a instabilidade numérica. Outro
plano de corte bem estabelecido na literatura é o corte por arredondamento
inteiro misto~\cite{wosley88}, que foi demonstrado ser uma forma genérica para
planos de cortes. Apesar de sua inviabilidade, os planos de corte possuem grande
importância teórica.

\cite{little1963algorithm} e
\cite{land2010automatic} propuseram um método de enumeração implícita que veio a
ser chamado de \textit{Branch and Bound}(BnB). O BnB consiste em dividir um
espaço de busca $S$ em subespaços $S_1, S_2, \ldots, S_n$ de modo que
$S_1 \cup S_2 \cup \ldots \cup S_n = S$. Para cada espaço gerado é calculado
um limite superior e inferior de otimalidade, e os espaços são divididos
novamente. Baseado nos limites, o BnB tende a seguir por regiões que levam
a melhores resultados e elimina regiões infrutíferas. Um exemplo de
limite superior é a relaxação linear, e um limite inferior é qualquer solução
factível para um problema. Se uma região $S_1$ possui um limite inferior de
$z = 22.4$ e uma região $s_2$ possui limite superior de $z = 21.0$, esta região pode
ser descartada desde que seja encontrado uma solução factível em $S_1$.
O desempenho do BnB está diretamente ligado à distância entre os limites inferiores
e superiores, quanto menor a distância melhor o desempenho tende a ser.

Uma das características dos planos de corte, é que eles podem reduzir a
distância entre a solução ótima da relaxação linear e a solução ótima
da PLI. Sendo assim, pode-se incluir a geração de planos de cortes no processo
de solução do BnB, melhorando assim seus limites. O BnB que utiliza planos de
cortes é denominado de \textit{Branch and Cut} (BnC).

\section{Geração de Colunas}

Na programação linear inteira existem problemas que tornam-se inviáveis de serem
resolvidos puramente com os métodos acima mencionados. Tomemos o problema
do caixeiro viajante(TSP) como exemplo. Em~\eqref{stsp} pode-se observar uma das
muitas possíveis modelagens para o problema do TSP. A restrição~\eqref{stsp3} é
o que difere o TSP de um problema de designação, e é o que torna o TSP um
problema $\mathcal{NP}$-difícil, pois esta restrição insere uma quantidade
fatorial de restrições no modelo do TSP. Em~\eqref{fat_tsp} temos uma fórmula
que descreve o número de restrições que~\eqref{stsp3} insere. Portanto,
é necessário que se utilize um procedimento denominado de geração de planos de cortes
para lidar com esta grandeza de restrições.

\begin{equation} \label{fat_tsp}
    \sum_{k=2}^{|A|-1} \binom{|A|}{k}!
\end{equation}


\begin{subequations}
    \label{stsp}
    \begin{align} \label{func_tsp2}
        z &= min \sum_{(i,j) \in A} c_{ij}x_{ij}
        \intertext{sujeito à:}
        & \sum_{i \in V}     x_{ij} =  1          & \forall    j \in V, i \neq j                   & \label{stsp1} \\
        & \sum_{j \in V}     x_{ij} =  1          & \forall    i \in V, i \neq j                   & \label{stsp2} \\
        & \sum_{(i,j) \in A} x_{ij} = |S|-1       & \forall \; S \subset V, 2 \leq |S| \leq |V|-1  & \label{stsp3} \\
        & 0 \leq x_{ij} \leq 1                    & \forall    (i,j) \in E                         &
    \end{align}
\end{subequations}

Considerando o modelo genérico de PLI~\eqref{glinhas} a seguir:

\begin{subequations} \label{glinhas}
    \begin{align} \label{}
        \text{maximizar} \: z &= cx
        \intertext{sujeito à:}
        \label{} Ax &= b \\
        \label{} x &\geq 0
    \end{align}
\end{subequations}

pode-se escolher um conjunto arbitrário de restrições tal que
$\tilde{A} \subseteq A$ e $\tilde{b} \subseteq b$,
formando um novo problema de PLI~\eqref{glinhas2}.

\begin{subequations} \label{glinhas2}
    \begin{align} \label{}
        \text{maximizar} \: z &= cx
        \intertext{sujeito à:}
        \label{} \tilde{A}x &= \tilde{b} \\
        \label{} x &\geq 0
    \end{align}
\end{subequations}

Tem-se que~\eqref{glinhas2} é o problema apresentado em~\eqref{glinhas}, porém com um conjunto
reduzido de restrições, que pode ser resolvido mais facilmente. No entanto,
a solução ótima $\tilde{x}^*$ não necessariamente irá satisfazer
a~\eqref{glinhas}. Então é necessário que verifique-se qual das restrições
são violadas e estas devem ser inseridas em~\eqref{glinhas2}, que deve ser
resolvido novamente. Este verificação é conhecido como problema da separação,
que é $\mathcal{NP}$-completo, conforme demonstrado
em~\cite{nemhauser1988integer}.
%Apesar da dificuldade do problema da separação,
%resolver o PLI com um conjunto reduzido de restrições e resolver o problema da
%separação para descobrir quais restrições foram violadas tende a ser mais rápido.

O problema da separação pode ser formulado como outro problema de PLI, formulado de
modo dual, que indica qual é a restrição mais violada dentre todas. Esta
restrição é então adicionada ao problema, que é re-otimizado e pode ter
outras variáveis violadas. O processo é repetido até que não existam mais
variáveis violadas.

A principal vantagem deste método é que o conjunto de restrições finais é
muito menor do que o total de restrições do problema original. E portanto, o
custo de resolver diversos problemas de separação e efetuar múltiplas
re-otimizações do problema original tende a ser mais rápida do que resolver
o problema original.

Existem ainda problemas onde o número de restrições é relativamente pequeno,
enquanto que o \textbf{número de variáveis é muito maior}. Um exemplo de problema com
este comportamento é o \textit{Cutting stock problem}. O problema consiste
em dado uma demanda de peças de tamanhos arbitrários e um estoque de peças de
tamanho fixo, determinar como cortar o estoque para obter as peças demandadas
com um desperdício mínimo. Sua formulação é dada a seguir:

\begin{subequations}\label{cuttingsp}
    \begin{align}
        z = min \sum_{j \in J} c_{j}x_{ij} \\
        \sum_{j \in J} a_{ij} x_j \geq b_j, \forall i \in I \\
        x_j \in \mathbb{Z}^+
    \end{align}
\end{subequations}

onde $c_j$ corresponde a sobra de utilizar-se o corte $j$, $b_i$ é a demanda
para peças do tipo $i$, e $a_{j}$ corresponde a um padrão de corte.
Considere o seguinte exemplo: Deseja-se peças de tamanho $3, 4, \text{e } 5$
cortados a partir de um tubo de tamanho $10$. Alguns padrões válidos de corte
são: $(5, 5)$, $(5, 4)$, $(3, 3, 3)$, etc.

A matriz $A$ terá dimensões $m \times n$, onde $m$ é o número de diferentes
peças que são necessários e $n$ é o total de combinações de como se pode
efetuar os cortes. Conforme se aumenta o valor de $m$, $n$ cresce
exponencialmente.

Considerando que no simplex o número de variáveis básicas é limitado pelo
número de restrições, em um problema onde existe uma quantidade muito maior
de colunas do que linhas boa parte do tempo de solução seria gasto processando
variáveis que ao fim teriam o seu custo fixado em $0$. Portanto, a maioria
das colunas não são necessárias para obter-se o resultado final do problema.

A solução de problemas com muitas variáveis funciona de modo análogo ao com
muitas restrições. Baseado em um problema de PLI, é resolvido um segundo problema
que contem apenas um conjunto de variáveis. Enquanto que para muitas restrições
a solução era testada para detectar restrições violadas, para problemas com
muitas variáveis a solução é testada para determinar sua otimalidade.

Utilizando-se a solução ótima $\tilde{x}^*$ do problema reduzido e sua solução
ótima dual $\tilde{y}^*$ pode-se testar $\tilde{y}^*$ por restrições violadas
no problema dual completo. Portanto, adicionar restrições no problema dual
corresponde a adicionar colunas (e variáveis) no problema primal. Este processo
de resolver o problema primal reduzido e detectar restrições duais pode ser
repetido até que se obtenha uma solução $\tilde{x}^*$ que não viole restrições
no problema dual.

Neste capitulo foi apresentado os principais conceitos teóricos necessários para
o entendimento do problema a ser abordado nos próximos capítulos deste trabalho.
Discutiu-se as principais representações de problemas de programação linear e
programação linear inteira, suas principais propriedades e métodos de solução.
Por fim introduziu-se o conceito de utilizar custo reduzido e o problema dual
para determinar restrições violadas ou colunas que podem melhorar a função objetivo.

\chapter{O problema de cobertura, particionamento e escalonamento de tripulação}

A alocação de tripulação (CSP) consiste de um dos principais problemas para se tratar de um
ponto de vista operacional dentro do contexto de empresas de transporte aéreo e terrestre.
Segundo reportado por~\cite{zeren2012improved}, os gastos com tripulação consistem
na segunda maior fonte de despesas, atrás apenas dos gastos de combustíveis. Portanto,
a alocação de tripulação consiste de um ponto de vista operacional um lugar onde a
otimização pode oferecer reduções significativas nos gastos, enquanto que de um ponto
de vista acadêmico é um problema difícil de resolver, com grandes instâncias e uma
aplicabilidade prática.

O CSP pode ser definido como criar e designar jornadas para um conjunto de tripulantes de
modo a cobrir todas as tarefas que devem ser realizadas. Conforme já definido no
capítulo~\ref{cap1}, uma \textbf{tarefa} é definida como sendo uma ação que deve
ser realizada por uma tripulação. Durante uma tarefa a tripulação dedica-se integralmente
a mesma durante um período de tempo fixo. Uma \textbf{jornada} consiste em um conjuntos de
tarefas que dever ser cobertas por uma tripulação. A cada jornada é atribuído um
custo operacional, e a geração de jornadas esta limitada por leis trabalhistas e
sindicais. O principal objetivo do CSP é determinar o conjunto de jornadas que
cobre todas as tarefas, reduzindo o custo operacional, respeitando as restrições
impostas em relação as jornadas de trabalho e designando apenas uma tripulação
por jornada.

Conhecendo-se um número suficientemente grande de jornadas (grande o suficiente
para poder cobrir todas as tarefas de modo factível) pode-se modelar o CSP como
um problema de \textit{set covering problem} (SCP) ou \textit{set partitioning problem} (SPP),
dependendo de é desejável que uma jornada seja coberta mais de uma vez. No restante
deste capitulo é discutido os problemas de cobertura e particionamento assim como uma
abordagem mais detalhada do CSP.

\section{Crew Scheduling Problem}

Esta seção é dedicada a explicar a modelagem e as instâncias utilizadas neste
trabalho. Utilizou-se as instâncias presentes na \textit{OR-Library}~\cite{beasley1990or},
apresentadas inicialmente em~\cite{beasley1996tree}. Este trabalho utiliza a versão
de Junho de 2016 das instâncias.

Existem 10 problemas de CSP presentes na \textit{OR-Library}, sendo que todos
eles são utilizados como objeto de estudo neste trabalho. Cada instância é apresentada
no seguinte formato: Número de tarefas($N$); Tempo limite de uma jornada; para cada
$i\text{ em }\{1, \ldots, N\}$: tempo de início, tempo de termino; Para cada par
$(i, j)$ de arestas, onde $j$ começa após o término de $i$, o custo da transição
de $i$ para $j$. O problema pode ser representado em um grafo, conforme sera visto
adiante. Beasley, no seu artigo considera um número fixo de jornadas para a solução final.
Este número fixo corresponderia a tripulação disponível operar. Apesar desta informação
não estar disponível nas instâncias, ela foi utilizada.

Existem diversos modos de se formular matematicamente as instâncias apresentadas, uma
delas é apresentada no artigo onde foram propostas as instâncias. Neste trabalho utilizou-se
uma formulação baseada em particionamento de conjuntos. Para que se possa obter a
resposta ótima para qualquer instância de CSP modelada como SPP, o modo mais simples consiste
em enumerar todas as jornadas viáveis. Algo que torna-se impraticável rapidamente, considerando
que o simples fato de enumerar-se todas as jornadas leva um tempo exponencial em relação ao
número de tarefas. No entanto este modelo é a base para a solução por geração de colunas.

\begin{figure}[!htb]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{c c c}
            5  & 14 &   \\% \hline
            1  & 7  &   \\% \hline
            1  & 4  &   \\% \hline
            11 & 14 &   \\% \hline
            6  & 10 &   \\% \hline
            11 & 15 &   \\% \hline
            1  & 3  & 5 \\% \hline
            1  & 5  & 6 \\% \hline
            2  & 3  & 3 \\% \hline
            2  & 4  & 4 \\% \hline
            2  & 5  & 6 \\% \hline
            4  & 3  & 4 \\% \hline
            4  & 5  & 3 \\% \hline
        \end{tabular}
        \captionof{table}{Instância do CSP}
        \label{tab_graph_csp}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{../figuras/graph.pdf}
        \captionof{figure}{Jornadas modeladas como grafo}
        \label{fig_graph_csp}
    \end{minipage}
\end{figure}

Na figura~\ref{fig_graph_csp} é apresentado um grafo relativo a instância apresentada
na tabela~\ref{tab_graph_csp}, no modelo apresentado acima. A enumeração de todas as jornadas
factíveis e seus respectivos custo e durações são apresentadas na tabela~\ref{tab_gragh_csp_enum}.
Utilizando-se estas informações, é possível utilizar um problema de particionamento de conjuntos
para resolver o CSP. A formulação genéria de um SPP é apresentada em~\eqref{spppp}. Tem-se que $c_j$
é o custo associado a $j$-ésima jornada, $x_j = 1$ se e somente se a jornada $j$ é utilizada. Tem-se
ainda que $a_{ij} = 1$ se e somente se a $i$-ésima tarefa é coberta pela $j$-ésima jornada.
Em~\eqref{csp_1} tem-se a codificação para uma PLI do SPP associado ao problema de CSP apresentado
na tabela~\ref{tab_graph_csp}. A função objetivo~\eqref{spp2} consiste no produto escalar entre o vetor de custos
de cada jornada e as variáveis de decisão $x$. A restrição~\eqref{spp22} faz com que 

\begin{table}[htpb]
    \centering
    \begin{tabular}{l l l}
        Jornada                           & Custo & Duração \\
        1 $\rightarrow$ 3                 & 5     & 13 \\
        1 $\rightarrow$ 5                 & 6     & 14 \\
        2 $\rightarrow$ 3                 & 3     & 13 \\
        2 $\rightarrow$ 4                 & 4     & 9  \\
        2 $\rightarrow$ 4 $\rightarrow$ 5 & 7     & 14 \\
        2 $\rightarrow$ 4 $\rightarrow$ 3 & 8     & 13 \\
        2 $\rightarrow$ 5                 & 6     & 14 \\
    \end{tabular}
    \caption{Enumeração de todas as jornadas viáveis}
    \label{tab_gragh_csp_enum}
\end{table}

\begin{subequations}
    \label{spppp}
    \begin{align}
        \label{spp2}  \text{min} \: \sum_{j \in J} c_j x_j \\
        \label{spp22} \sum_{j \in J} a_{tj} x_j = 1, \forall t \in T \\
        \label{spp23} \sum_{j \in J}        x_j = 2, \forall t \in T \\
        \label{spp24} x_j \in \{0, 1\}, \forall j \in J
    \end{align}
\end{subequations}

\begin{equation}
    \label{csp_1}
    A = \begin{pmatrix}
        1 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 1 & 1 & 1 & 1 \\
        1 & 0 & 1 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1 & 1 & 1 & 0 \\
        0 & 1 & 0 & 0 & 1 & 0 & 1 \\
    \end{pmatrix}, \quad
    b = \begin{pmatrix}
        1 \\
        1 \\
        1 \\
        1 \\
        1 \\
    \end{pmatrix}, \quad
    c^T = \begin{pmatrix}
        5 \\
        6 \\
        3 \\
        4 \\
        7 \\
        8 \\
        6 \\
    \end{pmatrix}
\end{equation}


%\section{Set Partitioning Problem e Set Covering Problem}

%\begin{align}
    %\label{spp2} \text{min} \: \sum_{j \in J} c_j x_j \\
    %\label{spp22} \sum_{j \in J} a_{tj} x_j = 1, \forall t \in T \\
    %\label{spp23} x_j \in \{0, 1\}, \forall j \in J
%\end{align}

%\begin{align}
    %\label{scp2} \text{min} \: \sum_{j \in J} c_j x_j \\
    %\label{scp22} \sum_{j \in J} a_{tj} x_j \ge 1, \forall t \in T \\
    %\label{scp23} x_j \in \{0, 1\}, \forall j \in J
%\end{align}

